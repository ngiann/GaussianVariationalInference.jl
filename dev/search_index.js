var documenterSearchIndex = {"docs":
[{"location":"technicaldescription/#Technical-description","page":"Technical description","title":"Technical description","text":"","category":"section"},{"location":"technicaldescription/#Variational-inference-via-the-re-parametrisation-trick","page":"Technical description","title":"Variational inference via the re-parametrisation trick","text":"","category":"section"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"Our goal is to approximate the true (unnormalised) posterior  distribution p(thetamathcalD) with a Gaussian q(theta) = mathcalN(thetamuSigma) by  maximising the expected lower bound:","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"int q(theta) log p(x theta) dtheta + mathcalHq,","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"also known as the ELBO. The above integral is approximated with as a Monte carlo average over S number of samples:","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"frac1S sum_s=1^S log p(x theta_s) + mathcalHq.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"Using the reparametrisation trick, we re-introduce the variational parameters that we need to optimise:","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"frac1S sum_s=1^S log p(x mu + C z_s) + mathcalHq,","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"where z_ssimmathcalN(0I) and C is a matrix root of Sigma, i.e. CC^T = Sigma.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"By optimising the approximate lower bound with respect to the variational parameters mu and C we obtain the approximate posterior q(theta) = mathcalN(thetamuCC^T) that offers the best Gaussian approximation to true posterior p(thetamathcalD).","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"The number of samples S in the above description, can be controlled via the option S when calling VI. ","category":"page"},{"location":"technicaldescription/#In-more-detail","page":"Technical description","title":"In more detail","text":"","category":"section"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"This package implements variational inference using the re-parametrisation trick. Contrary to other flavours of this method, that repeatedly draw new samples z_s at each iteration of the optimiser, here a large number of samples z_s is drawn  at the start and is kept fixed throughout the execution of the algorithm[1]. This avoids the difficulty of working with a noisy gradient and allows the use of optimisers like LBFGS. Using LBFGS, does away with the typical requirement of tuning learning rates (step sizes). However, this comes at the expense of risking overfitting to the samples z_s that happened to be drawn at the start. The package provides a mechanism for monitoring potential overfitting[2] via the options  Stest and test_every. Because of fixing the samples  z_s, the algorithm doesn't not enjoy the speed of optimisation via stochastic gradient. As a consequence,  the present package is recommented for problems with relatively few parameters, e.g. 2-20 parameters perhaps.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"The work was independently developed and published here (Arxiv link). Of course, the method has been widely popularised by the works Doubly Stochastic Variational Bayes for non-Conjugate Inference and Auto-Encoding Variational Bayes. The method indepedently appeared earlier in Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression and later in A comparison of variational approximations for fast inference in mixed logit models and perhaps in other publications too...","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"[1]: See paper, Algorithm 1.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"[2]: See paper, Section 2.3.","category":"page"},{"location":"moreoptions/#More-options","page":"More options","title":"More options","text":"","category":"section"},{"location":"moreoptions/#Specifying-gradient-options","page":"More options","title":"Specifying gradient options","text":"","category":"section"},{"location":"moreoptions/","page":"More options","title":"More options","text":"Function VI allows the user to obtain a Gaussian approximation with minimal requirements.  The user only needs to code a function logp that implements the log-posterior, provide an initial starting point x₀ and call:","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"# log-posterior is a Gaussian with zero mean and unit covariance.\n# Hence, our approximation should be exact in this example.\nlogp(x) = -sum(x.*x) / 2\n\n# implicitly specifies that the log-posterior is 5-dimensional\nx₀ = randn(5)\n\n# obtain approximation\nq, logev = VI(logp, x₀, S = 200, iterations = 10_000, show_every = 200)\n\n# Check that mean is close to zero and covariance close to identity.\n# mean and cov are re-exported function from Distributions.jl\nmean(q)\ncov(q)","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"However, providing a gradient for logp can speed up the computation in VI.","category":"page"},{"location":"moreoptions/#Gradient-free-mode","page":"More options","title":"➤  Gradient free mode","text":"","category":"section"},{"location":"moreoptions/","page":"More options","title":"More options","text":"Specify by gradientmode = :gradientfree.","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"If no options relating to the gradient are specified, i.e. none of the options gradientmode or gradlogp is specified, VI will by default use internally the Optim.NelderMead optimiser that does not need a gradient.  ","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"The user can explicitly specify that the algorithm should use the gradient free Optim.NelderMead optimisation algorithm by setting gradientmode = :gradientfree.","category":"page"},{"location":"moreoptions/#Automatic-differentiation-mode","page":"More options","title":"➤  Automatic differentiation mode","text":"","category":"section"},{"location":"moreoptions/","page":"More options","title":"More options","text":"Specify by gradientmode = :forward.","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"If logp is coding a differentiable function, then its gradient can be conveniently computed using automatic differentiation. By specifying gradientmode = :forward, function VI will internally use ForwardDiff to calculate the gradient of logp. In this case, VI will use internally the Optim.LBFGS optimiser.","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"q, logev = VI(logp, x₀, S = 200, iterations = 30, show_every = 1, gradientmode = :forward)","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"We note that with the use of gradientmode = :forward we arrive in fewer iterations to a result than in the gradient free case.","category":"page"},{"location":"moreoptions/#Gradient-provided","page":"More options","title":"➤  Gradient provided","text":"","category":"section"},{"location":"moreoptions/","page":"More options","title":"More options","text":"Specify by gradientmode = :provided.","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"The user can provide a gradient for logp via the gralogp option:","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"# Let us calculate the gradient explicitly\ngradlogp(x) = -x\n\nq, logev = VI(logp, x₀, gradlogp = gradlogp, S = 200, iterations = 30, show_every = 1, gradientmode = :provided)","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"In this case, VI will use internally the Optim.LBFGS optimiser. Again in this case we arrive in fewer iterations to a result than in the gradient free case.","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"note: Note\nEven if a gradient has been explicitly provided via the gralogl option, the user still needs to specify gradientmode = :provided to instruct VI to use the provided gradient.","category":"page"},{"location":"moreoptions/#Evaluating-the-lower-bound-on-test-samples","page":"More options","title":"Evaluating the lower bound on test samples","text":"","category":"section"},{"location":"moreoptions/","page":"More options","title":"More options","text":"The options S specifies the number of samples to use when approximating the expected lower bound, see Technical description. The higher the value we use for S, the better the approximation will be, however, at a higher computational cost. The lower the value we use for S, the faster the computation will be, but the approximation may be poorer. Hence, when setting S we need to take this trade-off into account.","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"Function VI offers a mechanism that informs us whether the value S is set to a high enough value. This mechanism makes use of two options, namely Stest and test_every. Option Stest defines the number of test samplesused exclusively for evaluating (not optimising!) the Kullback-Leibler divergence every test_every number of iterations. Monitoring the Kullback-Leibler divergence in this way offers an effective way of detecting whether S has been set sufficiently high.","category":"page"},{"location":"moreoptions/","page":"More options","title":"More options","text":"Function VI will report test_every iterations the value of ....","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"VI\nexampleproblem1","category":"page"},{"location":"reference/#ApproximateVI.VI","page":"Reference","title":"ApproximateVI.VI","text":"Basic use:\n\nq, logev = VI(logp, μ, σ²=0.1; S = 100, iterations = 1, show_every = -1)\n\nReturns approximate Gaussian posterior and log evidence.\n\nArguments\n\nA description of only the most basic arguments follows.\n\nlogp is a function that expresses the (unnormalised) log-posterior, i.e. joint log-likelihood.\nμ is the initial mean of the approximating Gaussian posterior.\nσ² specifies the initial covariance of the approximating Gaussian posterior as σ² * I . Default value is 0.1.\nS is the number of drawn samples that approximate the lower bound integral.\niterations specifies for how many iterations to run optimisation on the lower bound (elbo).\nshow_every: report progress every show_every number of iterations.\n\nOutputs\n\nq is the approximating posterior returned as a Distributions.MvNormal type\nlogev is the approximate log-evidence.\n\nExample\n\n# infer posterior of Bayesian linear regression, compare to exact result\njulia> using LinearAlgebra, Distributions\njulia> D = 4; X = randn(D, 1000); W = randn(D); β = 0.3; α = 1.0;\njulia> Y = vec(W'*X); Y += randn(size(Y))/sqrt(β);\njulia> Sn = inv(α*I + β*(X*X')) ; mn = β*Sn*X*Y; # exact posterior\njulia> posterior, logev = VI( w -> logpdf(MvNormal(vec(w'*X), sqrt(1/β)), Y) + logpdf(MvNormal(zeros(D),sqrt(1/α)), w), randn(D); S = 1_000, iterations = 15);\njulia> display([mean(posterior) mn])\njulia> display([cov(posterior)  Sn])\njulia> display(logev) # display negative log evidence\n\n\n\n\n\n","category":"function"},{"location":"reference/#ApproximateVI.exampleproblem1","page":"Reference","title":"ApproximateVI.exampleproblem1","text":"Synthetic two-dimensional problem\n\nExample\n\njulia> logp = exampleproblem1() # target distribution to approximate\njulia> q, logev = VI(logp, randn(2), S = 100, iterations = 10_000, show_every = 50)\njulia> using Plots # must be indepedently installed.\njulia> x = -3:0.02:3\njulia> contour(x, x, map(x -> exp(logp(collect(x))), Iterators.product(x, x))', fill=true, c=:blues, colorbar = false) # plot target\njulia> contour!(x, x, map(x -> pdf(q,(collect(x))), Iterators.product(x, x))', color=\"red\", alpha=0.3) # plot approximation q\n\n\n\n\n\n","category":"function"},{"location":"#ApproximateVI.jl","page":"Introduction","title":"ApproximateVI.jl","text":"","category":"section"},{"location":"#What's-this-package-for?","page":"Introduction","title":"What's this package for?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Use this package to approximate a posterior distribution with a Gaussian[1].","category":"page"},{"location":"#Basic-use","page":"Introduction","title":"Basic use","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Currently, the main function of interest this package exposes is VI. At the very minimum, the user needs to provide a function that codes the (unnormalised) log-posterior function.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Let's consider the following toy example:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using ApproximateVI\n\nlogp = exampleproblem1() # target log-posterior to approximate\nx₀ = randn(2)            # random initial mean for approximating Gaussian\nq, logev = VI(logp, randn(2), S = 100, iterations = 10_000, show_every = 50)\n\n# Plot target posterior, not log-posterior!\nusing Plots # must be indepedently installed.\nx = -3:0.02:3\ncontour(x, x, map(x -> exp(logp(collect(x))), Iterators.product(x, x))', fill=true, c=:blues)\n\n# Plot Gaussian approximation on top using red colour\ncontour!(x, x, map(x -> pdf(q,(collect(x))), Iterators.product(x, x))', color=\"red\", alpha=0.2)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"A plot similar to the one below should appear. (Image: exampleproblem1)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Options S above specifies the number of samples to use in order to approximate the variational lower bound, i.e. the objective that which minimised produces the best Gaussian approximation. The higher S is set the better, however, at a higher computational cost. The lower S the faster the method, but the riskier to produce a biased solution, see Technical description.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"[1]: Approximate Variational Inference Based on a Finite Sample of Gaussian Latent Variables, [Arxiv].","category":"page"}]
}
