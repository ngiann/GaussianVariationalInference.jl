var documenterSearchIndex = {"docs":
[{"location":"technicaldescription/#Technical-description","page":"Technical description","title":"Technical description","text":"","category":"section"},{"location":"technicaldescription/#Variational-inference-via-the-re-parametrisation-trick","page":"Technical description","title":"Variational inference via the re-parametrisation trick","text":"","category":"section"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"Our goal is to approximate the true (unnormalised) posterior  distribution p(thetamathcalD) with a Gaussian q(theta) = mathcalN(thetamuSigma) by  maximising the expected lower bound:","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"int q(theta) log p(x theta) dtheta + mathcalHq,","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"also known as the ELBO. The above integral is approximated with as a Monte carlo average over S number of samples:","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"frac1S sum_s=1^S log p(x theta_s) + mathcalHq.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"Using the reparametrisation trick, we re-introduce the variational parameters that we need to optimise:","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"frac1S sum_s=1^S log p(x mu + C z_s) + mathcalHq,","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"where z_ssimmathcalN(0I) and C is a matrix root of Sigma, i.e. CC^T = Sigma.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"By optimising the approximate lower bound with respect to the variational parameters mu and C we obtain the approximate posterior q(theta) = mathcalN(thetamuCC^T) that offers the best Gaussian approximation to true posterior p(thetamathcalD).","category":"page"},{"location":"technicaldescription/#In-more-detail","page":"Technical description","title":"In more detail","text":"","category":"section"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"This package implements variational inference using the re-parametrisation trick. Contrary to other flavours of this method, that repeatedly draw new samples z_s at each iteration of the optimiser, here a large number of samples z_s is drawn  at the start and is kept fixed throughout the execution of the algorithm[1]. This avoids the difficulty of working with a noisy gradient and allows the use of optimisers like LBFGS. A big advatange is that the use of LBFGS, does away with the typical requirement of tuning learning rates (step sizes). However, this comes at the expense of riskin g overfitting to the samples z_s that happened to be drawn at the start. The package provides a mechanism for monitoring potential overfitting[2] via the options  Stest and test_every. Because of fixing the samples  z_s, the algorithm doesn't not enjoy the speed of optimisation via stochastic gradient. As a consequence,  the present package is recommented for problems with relatively few parameters, e.g. 2-20 parameters perhaps.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"The work was independently developed and published here (Arxiv link). Of course, the method has been widely popularised by the works Doubly Stochastic Variational Bayes for non-Conjugate Inference and Auto-Encoding Variational Bayes. The method indepedently appeared earlier in Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression and  later in A comparison of variational approximations for fast inference in mixed logit models and perha ps in other publications too...","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"[1]: See paper, Algorithm 1.","category":"page"},{"location":"technicaldescription/","page":"Technical description","title":"Technical description","text":"[2]: See paper, Section 2.3.","category":"page"},{"location":"moreoptions/#More-options-here","page":"More options","title":"More options here","text":"","category":"section"},{"location":"moreoptions/#Specifying-gradient-options","page":"More options","title":"Specifying gradient options","text":"","category":"section"},{"location":"moreoptions/#Evaluating-the-lower-bound-on-test-samples","page":"More options","title":"Evaluating the lower bound on test samples","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"VI\nexampleproblem1","category":"page"},{"location":"reference/#ApproximateVI.VI","page":"Reference","title":"ApproximateVI.VI","text":"Basic use:\n\nq, logev = VI(logp, μ, σ²=0.1; S = 100, iterations = 1, show_every = -1)\n\nReturns approximate Gaussian posterior and log evidence.\n\nArguments\n\nA description of only the most basic arguments follows.\n\nlogp is a function that expresses the (unnormalised) log-posterior, i.e. joint log-likelihood.\nμ is the initial mean of the approximating Gaussian posterior.\nσ² specifies the initial covariance of the approximating Gaussian posterior as σ² * I . Default value is 0.1.\nS is the number of drawn samples that approximate the lower bound integral.\niterations specifies for how many iterations to run optimisation on the lower bound (elbo).\nshow_every: report progress every show_every number of iterations.\n\nOutputs\n\nq is the approximating posterior returned as a Distributions.MvNormal type\nlogev is the approximate log-evidence.\n\nExample\n\n# infer posterior of Bayesian linear regression, compare to exact result\njulia> using LinearAlgebra, Distributions\njulia> D = 4; X = randn(D, 1000); W = randn(D); β = 0.3; α = 1.0;\njulia> Y = vec(W'*X); Y += randn(size(Y))/sqrt(β);\njulia> Sn = inv(α*I + β*(X*X')) ; mn = β*Sn*X*Y; # exact posterior\njulia> posterior, logev = VI( w -> logpdf(MvNormal(vec(w'*X), sqrt(1/β)), Y) + logpdf(MvNormal(zeros(D),sqrt(1/α)), w), randn(D); S = 1_000, iterations = 15);\njulia> display([mean(posterior) mn])\njulia> display([cov(posterior)  Sn])\njulia> display(logev) # display negative log evidence\n\n\n\n\n\n","category":"function"},{"location":"reference/#ApproximateVI.exampleproblem1","page":"Reference","title":"ApproximateVI.exampleproblem1","text":"Synthetic two-dimensional problem\n\nExample\n\njulia> logp = exampleproblem1() # Target distribution to approximate\njulia> q, logev = VI(logp, randn(2), S = 100, iterations = 100, show_every = 5)\njulia> using PyPlot # PyPlot, or any other plotting package, must be indepedently installed.\njulia> x=-3:0.02:3\njulia> pcolor(x, x, map(x -> exp(logp(collect(x))), Iterators.product(x, x))') # plot target\n\n\n\n\n\n","category":"function"},{"location":"#ApproximateVI.jl","page":"Introduction","title":"ApproximateVI.jl","text":"","category":"section"},{"location":"#What's-this-package-for?","page":"Introduction","title":"What's this package for?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Use this package to approximate a posterior distribution with a Gaussian[1].","category":"page"},{"location":"#Basic-use","page":"Introduction","title":"Basic use","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Currently, the main function of interest this package exposes is VI. At the very minimum, the user needs to provide a function that codes the (unnormalised)log-posterior function.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Let's consider the following toy example:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using ApproximateVI\n\nlogp = exampleproblem1() # target log-posterior to approximate\nx₀ = randn(2)            # random initial mean for approximating Gaussian\nq, logev = VI(logp, x₀, S = 100, iterations = 100, show_every = 1)\n","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Options S above specifies the number of samples to use in order to approximate the variational lower bound, i.e. the objective that which minimised produces the best Gaussian approximation. The higher S is set the better, however, at a higher computational cost. The lower S the faster the method, but the riskier to produce a biased solution, see Technical description.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Let us plot the target posterior and the Gaussian approximation held in q:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using ApproximateVIUtilities # install this for auxiliary functionality\n","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"[1]: Approximate Variational Inference Based on a Finite Sample of Gaussian Latent Variables, [Arxiv].","category":"page"}]
}
