<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Technical description · GaussianVariationalInference</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GaussianVariationalInference logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GaussianVariationalInference</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../moreoptions/">More options</a></li><li class="is-active"><a class="tocitem" href>Technical description</a><ul class="internal"><li><a class="tocitem" href="#ELBO-maximisation"><span>ELBO maximisation</span></a></li><li><a class="tocitem" href="#Choosing-the-number-of-samples-S"><span>Choosing the number of samples <span>$S$</span></span></a></li><li><a class="tocitem" href="#Monitoring-ELBO-on-independent-test-set"><span>Monitoring ELBO on independent test set</span></a></li><li><a class="tocitem" href="#Relevant-options-in-VI"><span>Relevant options in <code>VI</code></span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Technical description</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Technical description</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/ngiann/GaussianVariationalInference.jl/blob/master/docs/src/technicaldescription.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Technical-background"><a class="docs-heading-anchor" href="#Technical-background">Technical background</a><a id="Technical-background-1"></a><a class="docs-heading-anchor-permalink" href="#Technical-background" title="Permalink"></a></h1><p>We provide details regarding the implemented algorithm. In brief, the algorithm maximises the variational lower bound, typically known as the ELBO, using the reparametrisation trick.</p><h2 id="ELBO-maximisation"><a class="docs-heading-anchor" href="#ELBO-maximisation">ELBO maximisation</a><a id="ELBO-maximisation-1"></a><a class="docs-heading-anchor-permalink" href="#ELBO-maximisation" title="Permalink"></a></h2><p>Our goal is to approximate the true (unnormalised) posterior  distribution <span>$p(\theta|\mathcal{D})$</span> with a Gaussian <span>$q(\theta) = \mathcal{N}(\theta|\mu,\Sigma)$</span>. To achieve this, we maximising the expected lower bound:</p><p><span>$\mathcal{L}(\mu,\Sigma) = \int q(\theta) \log p(\mathcal{D}, \theta) d\theta + \mathcal{H}[q]$</span>,</p><p>also known as the ELBO. The above integral is a lower bound to the marginal likelihood <span>$\int p(\mathcal{D},\theta) d\theta \geq \mathcal{L}(\mu,\Sigma)$</span> and is in general intractable. We can make progress by approximating it with as a Monte carlo average over <span>$S$</span> number of samples <span>$\theta_s\sim q(\theta)$</span>:</p><p><span>$\mathcal{L}(\mu,\Sigma) \approx \frac{1}{S} \sum_{s=1}^S \log p(\mathcal{D}, \theta_s) + \mathcal{H}[q]$</span>.</p><p>Due to the sampling, however, the variational parameters no longer appear in the above approximation. Nevertheless, it is possible to re-introduce them by rewriting the sampled parameters as:</p><p><span>$\theta_s = \mu + C z_s$</span>,</p><p>where <span>$z_s\sim\mathcal{N}(0,I)$</span> and <span>$C$</span> is a matrix root of <span>$\Sigma$</span>, i.e. <span>$CC^T = \Sigma$</span>. We refer collectively to all samples as <span>$Z = \{z_1 . . . , z_S \}$</span>. This is known as the reparametrisation trick. Now we are able to we re-introduce the variational parameters in the approximated ELBO:</p><p><span>$\mathcal{L}_{(FS)}(\mu,C,Z) = \frac{1}{S} \sum_{s=1}^S \log p(\mathcal{D}, \mu + C z_s) + \mathcal{H}[q]$</span>,</p><p>where the subscript <span>$FS$</span> stands for <em>finite sample</em>. We denote the approximate ELBO with <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> and make it explicit that it depends on the samples <span>$Z$</span>.  By maximising the approximate ELBO <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> with respect to the variational parameters <span>$\mu$</span> and <span>$C$</span> we obtain the approximate posterior <span>$q(\theta) = \mathcal{N}(\theta|\mu,CC^T)$</span> that is the best Gaussian approximation to true posterior <span>$p(\theta|\mathcal{D})$</span>.</p><h2 id="Choosing-the-number-of-samples-S"><a class="docs-heading-anchor" href="#Choosing-the-number-of-samples-S">Choosing the number of samples <span>$S$</span></a><a id="Choosing-the-number-of-samples-S-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-the-number-of-samples-S" title="Permalink"></a></h2><p>For large values of <span>$S$</span> the proposed approximate lower bound <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> approximates the true bound <span>$\mathcal{L}(\mu,\Sigma)$</span> closely. Therefore, we expect that optimising <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> will yield approximately the same variational parameters <span>$µ, C$</span> as the optimisation of the intractable true lower bound <span>$\mathcal{L}(\mu,\Sigma)$</span> would.</p><p>Here the samples <span>$z_s$</span> are drawn at start of the algorithm and are kept fixed throughout its execution. The proposed scheme exhibits some fluctuation as <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> depends on the random set of samples <span>$z_s$</span> that happened to be drawn at the start of the algorithm. Hence, for another set of randomly drawn samples <span>$z_s$</span> the function  <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> will be (hopefully only slightly) different. However, for large enough <span>$S$</span> the fluctuation due to <span>$z_s$</span> should be innocuous and optimising it should yield approximately the same variational parameters for any drawn <span>$z_s$</span>.</p><p>However, if on the other hand we choose a small value for <span>$S$</span>, then the variational parameters will overly depend on the small set of samples <span>$z_s$</span> that happened to be drawn at the beginning of the algorithm. As a consequence, <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> will approximate <span>$\mathcal{L}(\mu,\Sigma)$</span> poorly, and the resulting posterior <span>$q(\theta)$</span> will also be a poor approximation to the true posterior. <span>$p(\theta|\mathcal{D})$</span>. Hence, the variational parameters will be overadapted to the small set of samples <span>$z_s$</span> that happened to be drawn. Naturally, the question arises of how to choose a large enough <span>$S$</span> in order avoid the sitatuation where <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> over-adapts the  variational parameters to the samples <span>$z_s$</span>. </p><h2 id="Monitoring-ELBO-on-independent-test-set"><a class="docs-heading-anchor" href="#Monitoring-ELBO-on-independent-test-set">Monitoring ELBO on independent test set</a><a id="Monitoring-ELBO-on-independent-test-set-1"></a><a class="docs-heading-anchor-permalink" href="#Monitoring-ELBO-on-independent-test-set" title="Permalink"></a></h2><p>A practical answer to diagnosing whether a sufficiently high number of samples <span>$S$</span> has been chosen, is the following: at the beginning of the algorithm we draw a second independent set of samples <span>$Z^\prime =\{ z_1^\prime, z_2^\prime, \dots, z_S^\prime\}$</span> where <span>$S^\prime$</span> is preferably a number larger than <span>$S$</span>. </p><p>At each (or every few) iteration(s) we monitor the quantity <span>$\mathcal{L}_{(FS)}(\mu,C,Z^\prime)$</span> on the independent sample set <span>$Z^\prime$</span>. If the variational parameters are not overadapting to the <span>$Z$</span>, then we should see that as the lower bound <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span>  increases, the quantity <span>$\mathcal{L}_{(FS)}(\mu,C,Z^\prime)$</span>  should also display a tendency to increase. If on the other hand the variational parameters are overadapting to  <span>$Z$</span>, then though <span>$\mathcal{L}_{(FS)}(\mu,C,Z)$</span> is increasing, we will notice that <span>$\mathcal{L}_{(FS)}(\mu,C,Z^\prime)$</span>  is actually deteriorating. This is a clear sign that a larger <span>$S$</span> is required.</p><p>The described procedure is reminiscent of monitoring the generalisation performance of a learning algorithm on a validation set during training. A significant difference, however, is that while validation sets are typically of limited size, here we can set <span>$S^\prime$</span> arbitrarily large. In practice, one may experiment with such values as e.g. <span>$S^\prime = 2S$</span> or  <span>$S^\prime = 10S$</span>. We emphasise that the samples in <span>$Z^\prime$</span> are not used to optimise ELBO.</p><h2 id="Relevant-options-in-VI"><a class="docs-heading-anchor" href="#Relevant-options-in-VI">Relevant options in <code>VI</code></a><a id="Relevant-options-in-VI-1"></a><a class="docs-heading-anchor-permalink" href="#Relevant-options-in-VI" title="Permalink"></a></h2><p>This package implements variational inference using the re-parametrisation trick. Contrary to other flavours of this method, that repeatedly draw <span>$S$</span> new samples <span>$z_s$</span> at each iteration of the optimiser, here we draw at the start  a large number <span>$S$</span> of samples <span>$z_s$</span> and keep them fixed throughout the execution of the algorithm<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>. This avoids the difficulty of working with a noisy gradient and allows the use of optimisers like <a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/">LBFGS</a>. Using LBFGS, does away with the typical requirement of tuning learning rates (step sizes). However, this comes at the expense of risking overfitting to the samples <span>$z_s$</span> that happened to be drawn at the start. Because of fixing the samples  <span>$z_s$</span>, the algorithm doesn&#39;t not enjoy the same scalability as variational inference with stochastic gradient does. As a consequence,  the present package is recommented for problems with relatively few parameters, e.g. 2-20 parameters perhaps.</p><p>As explained in the previous section, one may monitor the approximate ELBO on an independent set of samples <span>$Z^\prime$</span> of size <span>$S^\prime$</span>. The package provides a mechanism for monitoring potential overfitting<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> via the options <code>Stest</code> and <code>test_every</code>. Options <code>Stest</code> set the number of test samples <span>$S^\prime$</span> and <code>test_every</code> specifies how often we should monitor the approximate ELBO on <span>$Z^\prime$</span> by evaluating <span>$\mathcal{L}_{(FS)}(\mu,C,Z^\prime)$</span>. Please consult <a href="../moreoptions/#Evaluating-the-ELBO-on-test-samples">Evaluating the ELBO on test samples</a> and the example <a href="../examples/#Monitoring-ELBO">Monitoring ELBO</a>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Whenever option <code>Stest</code> is set, <code>test_every</code> must be set to.</p></div></div><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><p>The work was independently developed and published <a href="https://doi.org/10.1007/s10044-015-0496-9">here</a>, <a href="https://arxiv.org/pdf/1906.04507.pdf">(Arxiv link)</a>. Of course, the method has been widely popularised by the works <a href="http://proceedings.mlr.press/v32/titsi as14.pdf">Doubly Stochastic Variational Bayes for non-Conjugate Inference</a> and <a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a>. The method seems to have appeared earlier in <a href="https://arxiv.org/abs/1206.6679">Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression</a> and again later in <a href="https://link.springer.com/article/10.1007%2Fs00180-015-0638-y">A comparison of variational approximations for fast inference in mixed logit models</a> and perhaps in other publications too...</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>See <a href="https://arxiv.org/pdf/1906.04507.pdf">paper</a>, Algorithm 1.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>See <a href="https://arxiv.org/pdf/1906.04507.pdf">paper</a>, Section 2.3.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../moreoptions/">« More options</a><a class="docs-footer-nextpage" href="../examples/">Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 25 January 2023 14:44">Wednesday 25 January 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
