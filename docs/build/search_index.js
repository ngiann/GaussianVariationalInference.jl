var documenterSearchIndex = {"docs":
[{"location":"interface.html#User-interface","page":"Documentation","title":"User interface","text":"","category":"section"},{"location":"interface.html","page":"Documentation","title":"Documentation","text":"VI","category":"page"},{"location":"interface.html#ApproximateVI.VI","page":"Documentation","title":"ApproximateVI.VI","text":"Basic use:\n\nq, logev = VI(logl, μ, [σ²=0.1]; S = 100, iterations = 1, show_every = -1)\n\nReturns approximate Gaussian posterior and log evidence.\n\nArguments\n\nA description of only the most basic arguments follows. Arguments in brackets are optional.\n\nlogl is a function that expresses the joint log-likelihood\nμ is the initial mean of the approximating Gaussian posterior.\nσ² specifies the initial covariance as σ² * I of the approximating Gaussian posterior. Default value is 0.1.\nS is the number of drawn samples that approximate the lower bound integral.\nshow_every: report progress every show_every number of iterations.\n\nOutputs\n\nq is the approximating posterior returned as a Distributions.MvNormal type\nlogev is the approximate log-evidence.\n\nExample\n\n# infer posterior of Bayesian linear regression, compare to exact result\njulia> using LinearAlgebra, Distributions\njulia> D = 4; X = randn(D, 1000); W = randn(D); β = 0.3; α = 1.0;\njulia> Y = vec(W'*X); Y += randn(size(Y))/sqrt(β);\njulia> Sn = inv(α*I + β*(X*X')) ; mn = β*Sn*X*Y; # exact posterior\njulia> posterior, logev = VI( w -> logpdf(MvNormal(vec(w'*X), sqrt(1/β)), Y) + logpdf(MvNormal(zeros(D),sqrt(1/α)), w), randn(D); S = 1_000, iterations = 15);\njulia> display([mean(posterior) mn])\njulia> display([cov(posterior)  Sn])\njulia> display(logev) # display negative log evidence\n\n\n\n\n\n","category":"function"},{"location":"technicaldescription.html#Technical-description","page":"Tecnical description","title":"Technical description","text":"","category":"section"},{"location":"technicaldescription.html#Variational-inference-via-the-re-parametrisation-trick","page":"Tecnical description","title":"Variational inference via the re-parametrisation trick","text":"","category":"section"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"Our goal is to approximate the true (unnormalised) posterior  distribution p(thetamathcalD) with a Gaussian q(theta) = mathcalN(thetamuSigma) by maximising the expected lower bound:","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"int q(theta) log p(x theta) dtheta + mathcalHq","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"also known as the ELBO. The above integral is approximated with as a Monte carlo average over S number of samples:","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"frac1S sum_s=1^S log p(x theta_s) + mathcalHq","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"Using the reparametrisation trick, we re-introduce the variational parameters that we need to optimise:","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"frac1S sum_s=1^S log p(x mu + C z_s) + mathcalHq","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"where z_ssimmathcalN(0I) and C is a matrix root of Sigma, i.e. CC^T = Sigma.","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"By optimising the approximate lower bound with respect to the variational parameters mu and C we obtain the approximate posterior q(theta) = mathcalN(thetamuCC^T) that offers the best Gaussian approximation to true posterior p(thetamathcalD).","category":"page"},{"location":"technicaldescription.html#In-more-detail","page":"Tecnical description","title":"In more detail","text":"","category":"section"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"This package implements variational inference using the re-parametrisation trick. Contrary to other flavours of this method, that repeatedly draw new samples z_s at each iteration of the optimiser, here a large number of samples z_s is drawn at the start and is kept fixed throughout the execution of the algorithm[1]. This avoids the difficulty of working with a noisy gradient and allows the use of optimisers like LBFGS. A big advatange is that the use of LBFGS, does away with the typical requirement of tuning learning rates (step sizes). However, this comes at the expense of risking overfitting to the samples z_s that happened to be drawn at the start. The package provides a mechanism for monitoring potential overfitting[2] via the options Stest and test_every. Because of fixing the samples  z_s, the algorithm doesn't not enjoy the speed of optimisation via stochastic gradient. As a consequence, the present package is recommented for problems with relatively few parameters, e.g. 2-20 parameters perhaps.","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"The work was independently developed and published here (Arxiv link). Of course, the method has been widely popularised by the works Doubly Stochastic Variational Bayes for non-Conjugate Inference and Auto-Encoding Variational Bayes. The method indepedently appeared earlier in Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression and later in A comparison of variational approximations for fast inference in mixed logit models and perhaps in other publications too...","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"[1]: See paper, Algorithm 1.","category":"page"},{"location":"technicaldescription.html","page":"Tecnical description","title":"Tecnical description","text":"[2]: See paper, Section 2.3.","category":"page"},{"location":"moreoptions.html#More-options-here","page":"More options","title":"More options here","text":"","category":"section"},{"location":"index.html#What-is-this?","page":"Introduction","title":"What is this?","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"A Julia package for approximate Bayesian inference for non-conjugate probabilistic models[1].","category":"page"},{"location":"index.html#Basic-usage","page":"Introduction","title":"Basic usage","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The package is fairly easy to use. Currently, the only function of interest to the user is VI. At the very minimum, the user needs to provide a function that codes the joint log-likelihood function.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Consider approximating a target density given by a three-component mixture model:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using PyPlot # Must be indepedently installed. \n             # Needed for plotting, alternatively other packages can be used.\n\n# Define means for three-component Gaussian mixture model\n# All components are implicitly equally weighted and have unit covariance\nμ = [zeros(2), [2.5; 0.0], [-2.5; 0.0]]\n\n# Define log-likelihood\nlogp(θ) = log(exp(-0.5*sum((μ[1].-θ).^2)) + exp(-0.5*sum((μ[1].-θ).^2)) + exp(-0.5*sum((μ[3].-θ).^2)))","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"We will now approximate it with a Gaussian density. We need to pass to VI the log-likelihood function, a starting point for the mean of the approximate Gaussian posterior, as well as the number of fixed samples and the number of iterations we want to optimise the lower bound for:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"posterior, logevidence = VI(logp, randn(2); S = 100, iterations = 30)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This returns two outputs: the first one is the approximating posterior q(θ) of type MvNormal (see Distributions.jl). The second output is the approximate lower bound of type Float64.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Below we plot as contour plot the target unnormalised posterior distribution. We also plot the approximating posterior q(θ) as a blue ellipse:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"[1]: Approximate Variational Inference Based on a Finite Sample of Gaussian Latent Variables, [Arxiv].","category":"page"}]
}
